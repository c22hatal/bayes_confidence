{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantified Degree of Belief, Posterior Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the client has checked that the model assumptions are met using **Posterior Predictive Checks**, they can interpret the results of estimation and forecasting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once they have settled on their final model (!! after **Model Selection**, which we will discuss in a later notebook), we want them to correctly assess it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The client will have access to point estimates for model parameters and forecasts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are based on the mode of the parameters' posterior distribution.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The client will have access to measures of variability for the parameter estimates and forecast.  Finally, we will provide the client with tools to assess the results with statistical confidence, a quantified degree of belief.  These can include statistical tests and confidence intervals for the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will discuss what we will make available and justify why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus on model parameter estimation.  What we do with forecasting can be informed by what we discuss here, but those details need to be investigated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the moment, we are not using MCMC directly to estimate model parameters.  MCMC is being using for posterior predictive checks and statistical certification.\n",
    "\n",
    "For model estimation, MCMC can be a more computationally intensive alternative to what we discuss here.  In small sample size situations, MCMC can be much more accurate as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Approximation to the Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use numerical optimization to obtain the posterior mode $\\widehat{\\boldsymbol \\theta}$, maximizing the posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle \\pi(\\boldsymbol{\\theta}\\vert {\\bf x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior is proportional (where the scaling does not depend on $\\boldsymbol{\\theta}$) to the prior and likelihood (or density of the data).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle \\pi(\\boldsymbol{\\theta}\\vert {\\bf x}) \\propto L(\\boldsymbol{\\theta}\\vert {\\bf x}) \\pi(\\boldsymbol{\\theta})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in maximum likelihood, we directly maximize the log-posterior, $\\log \\pi(\\boldsymbol{\\theta}\\vert {\\bf x})$ because it is more stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as described in section 4.1 of BDA 3 (Bayesian Data Analysis 3rd edn A. Gelman, J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari and D. B. Rubin, 2013 Boca Raton, Chapman and Hallâ€“CRC), we can approximate $\\ln \\pi(\\boldsymbol{\\theta}\\vert {\\bf x})$ using a 2nd order Taylor Expansion around $\\widehat{\\boldsymbol \\theta}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle \\log \\pi(\\boldsymbol{\\theta}\\vert {\\bf x}) \\approx \\log \\pi(\\widehat{\\boldsymbol \\theta}\\vert{\\bf x}) + (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol \\theta} )^TS({\\boldsymbol \\theta})\\vert_{{\\boldsymbol \\theta}={\\widehat{\\boldsymbol \\theta}}} + \\frac{1}{2}(\\boldsymbol{\\theta} - \\widehat{\\boldsymbol \\theta} )^T H(\\widehat{\\boldsymbol \\theta}) (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol \\theta} )$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $S(\\boldsymbol{\\theta})$ is the score function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle S(\\boldsymbol{\\theta}) = \\frac{\\delta}{\\delta \\boldsymbol{\\theta}} \\log \\pi(\\boldsymbol{\\theta}\\vert {\\bf x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and $H(\\boldsymbol{\\theta})$ is the Hessian function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle H(\\boldsymbol{\\theta}) =  \\frac{\\delta}{\\delta \\boldsymbol{\\theta}^T} S(\\boldsymbol{\\theta}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that $\\widehat{\\boldsymbol \\theta}$ is in the interior of the parameter space (or support) of $\\boldsymbol{\\theta}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, $\\pi(\\boldsymbol{\\theta}\\vert {\\bf x})$ is a continuous function of $\\boldsymbol{\\theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the Hessian matrix, $H(\\boldsymbol{\\theta})$ is negative definite, so $-H(\\boldsymbol{\\theta})$ is positive definite.  This means that we can invert $-H(\\boldsymbol{\\theta})$ and get a matrix that is a valid covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these assumptions, as the sample size $n\\to\\infty$ the quadratic approximation for $\\log \\pi(\\boldsymbol{\\theta}\\vert {\\bf x})$ becomes more accurate.  At the posterior mode ${\\boldsymbol \\theta}={\\widehat{\\boldsymbol \\theta}}$, \n",
    "$\\log \\pi(\\boldsymbol{\\theta}\\vert {\\bf x})$ is maximized and $0=S({\\boldsymbol \\theta})\\vert_{{\\boldsymbol \\theta}={\\widehat{\\boldsymbol \\theta}}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this, we can exponentiate the approximation to get\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle \\pi(\\boldsymbol{\\theta}\\vert {\\bf x}) \\approx \\pi(\\widehat{\\boldsymbol \\theta}\\vert{\\bf x}) \\exp(\\frac{1}{2} (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol \\theta} )^T H(\\widehat{\\boldsymbol \\theta}) (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol \\theta} ))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for large $n$, the posterior distribution of ${\\boldsymbol \\theta}$ is approximately proportional to a multivariate normal density with mean $\\widehat{\\boldsymbol{\\theta}}$ and covariance $-H(\\widehat{\\boldsymbol{\\theta}})^{-1}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle {\\boldsymbol \\theta} \\vert x \\approx_D N(\\widehat{\\boldsymbol{\\theta}}, -H(\\widehat{\\boldsymbol{\\theta}})^{-1})$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another caveat for this result is that the prior should be proper, or at least lead to a proper posterior.  \n",
    "Our asymptotic results are depending on probabilities integrating to 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Constraints and Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization can be easier if the parameters are defined over the entire real line.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters that do not follow this rule are plentiful.  Variances are only positive.  Probabilities are in [0,1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stan provides the ease of optimizing parameters over the entire real line by creating unconstrained parameters.\n",
    "These are continuous functions of the constrained parameters, which may be defined on intervals of the real line.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the unconstrained version of a standard deviation parameter $\\sigma$ is $\\psi= \\log \\sigma$. $\\psi$ is defined over the entire real line.      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be useful for us to consider the constrained parameters as being functions of the unconstrained parameters.  \n",
    "\n",
    "So $\\sigma=exp(\\psi)$ is our constrained parameter of $\\psi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " So the posterior mode of the constrained parameters  ${\\boldsymbol{\\theta_c}}$  is $\\widehat{\\boldsymbol \\theta}_{\\boldsymbol c} = g(\\widehat{\\boldsymbol \\theta})$.  We will call $g$ the **constraint** function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use the delta method on $g$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first-order taylor approximation of $g({\\boldsymbol \\theta})$ at $\\widehat{\\boldsymbol \\theta}$ yields "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle g({\\boldsymbol \\theta}) \\approx g(\\widehat{\\boldsymbol \\theta}) + \\left\\{\\frac{\\delta}{\\delta \\widehat{\\boldsymbol{\\theta}}} g(\\widehat{\\boldsymbol{\\theta}})\\right\\} ({\\boldsymbol \\theta} - \\widehat{\\boldsymbol{\\theta}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remembering that $\\boldsymbol \\theta$ is approximately normal, the rules about linear transformations for multivariate normal random vectors tell us that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle {\\boldsymbol{\\theta_c}}\\vert x = g({\\boldsymbol \\theta}) \\vert x \\approx_D N \\left\\lbrack g(\\widehat{\\boldsymbol{\\theta}}), \\left\\{\\frac{\\delta}{\\delta \\widehat{\\boldsymbol{\\theta}}} g(\\widehat{\\boldsymbol{\\theta}})\\right\\}^T \\left\\{-H(\\widehat{\\boldsymbol{\\theta}})^{-1}\\right\\} \\left\\{\\frac{\\delta}{\\delta \\widehat{\\boldsymbol{\\theta}}} g(\\widehat{\\boldsymbol{\\theta}})\\right\\}\\right\\rbrack$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This involved a first-order approximation of $g$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier we used a second order approximation for taking the numeric derivative.  Why would we just do a first-order here?    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditionally the delta-method is taught and used as only a first-order method.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually the functions used in the delta method are not incredibly complex.  It is *good enough* to to use the first-order approximation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to use the normal approximation, we need $\\widehat{\\boldsymbol{\\theta}}$, $H(\\widehat{\\boldsymbol{\\theta}})^{-1}$, and $\\frac{\\delta}{\\delta \\widehat{\\boldsymbol{\\theta}}} g(\\widehat{\\boldsymbol{\\theta}})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, we use numerical optimization to get $\\widehat{\\boldsymbol{\\theta}}$.  Ideally, we would have analytic expressions for $H$ and the derivatives of $g$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyStan, no estimate of the Hessian is exposed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RStan estimates a Hessian using numerical differentiation on the analytic gradient.  See:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/27202395/how-do-i-get-standard-errors-of-maximum-likelihood-estimates-in-stan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://cran.r-project.org/web/packages/rstan/rstan.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform numerical differentiation in Python to get the Hessian and the gradient of the constraint function $g$.  This will be less accurate than an analytic expression, \n",
    "and will also normally be more computationaly intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But \"*Once you learn how to take one numeric derivative, you can take the numeric derivative of anything*\".  So using numerical differentiation is a very flexible technique that we can easily apply to all the models that we would use in Stan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So numeric derivatives can be very pragmatic, and flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you compute them?  Are they accurate?  We use the following reference as a guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. 2007. Numerical Recipes 3rd Edition: The Art of Scientific Computing (3rd. ed.). Cambridge University Press, USA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the definition of a derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f'(x) = \\displaystyle\\lim_{h\\to0}\\frac{f(x+h)-f(x)}{h}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To approximate $f'(x)$ numerically, couldn't we just plugin a small value for $h$ and compute the scaled difference? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes.  And that is basically what happens.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do do a little more work to choose $h$ and use a second-order approximation instead of a first-order.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the scaled difference is a first-order approximation by looking at the Taylor series expansion around $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taylor's theorem with remainder gives "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x+h)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "$ = f(x) + ((x+h)-x)f'(x) + .5((x+h)-x)^2 f''(\\epsilon)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ = f(x) + -h f'(x) + .5 h f''(\\epsilon)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\epsilon$ is between $x$ and $x+h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rearrange to get \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle\\frac{f(x+h)-f(x)}{h} - f'(x) = .5 h f''(\\epsilon)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is linear in h."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do second-order approximations for $f(x+h)$ and $f(x-h)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle f(x+h) = f(x) + ((x+h)-x)f'(x) + \\frac{((x+h)-x)^2 f''(x)}{2!} + \\frac{((x+h)-x)^3f'''(\\epsilon_1)}{3!}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle f(x-h) = f(x) + ((x-h)-x)f'(x) + \\frac{((x-h)-x)^2 f''(x)}{2!} + \\frac{((x-h)-x)^3f'''(\\epsilon_2)}{3!}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\epsilon_1$ is between $x$ and $x+h$ and $\\epsilon_2$ is between $x-h$ and $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle \\frac{f(x+h) - f(x-h)}{2h} - f'(x) = h^2 \\frac{f'''(\\epsilon_1)+ f'''(\\epsilon_2)}{12}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quadratic in h."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term takes equal input from both sides of $x$, so we call it a centered derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we choose a small value of $h$ and plug it into "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle \\frac{f(x+h) - f(x-h)}{2h}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to approximate $f'(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our derivation used a single input function $f$.  The idea applies to partial derivatives of multi-input functions as well.  The inputs that you aren't taking the derivative with respect to are treated as fixed parts of the function.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a Bandwidth, *h*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, second order approximation actually involves two sources of error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Roundoff error**, $\\epsilon_r$ arises from being unable to represent $x$ and $h$ or functions of them with exact binary represetation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle \\epsilon_r \\approx \\epsilon_f\\frac{\\mid{f(x)}\\mid}{h}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\epsilon_f$ is the fractional accuracy with which $f$ is computed.  This is generally machine accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\epsilon_f = \\mbox{np.finfo(float).eps}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remainder that we showed earlier, $\\displaystyle h^2 \\frac{f'''(\\epsilon_1)+ f'''(\\epsilon_2)}{12}$ is referred to as **truncation** error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimizing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle \\epsilon_r  + \\epsilon_t \\approx \\epsilon_f\\frac{\\mid{f(x)}\\mid}{h} + h^2 \\frac{f'''(\\epsilon_1)+ f'''(\\epsilon_2)}{12}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we obtain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle h \\sim \\epsilon_f^{1/3} \\left(\\frac{f}{f'''}\\right)^{1/3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\displaystyle \\left(\\frac{f}{f'''}\\right)^{1/3}$ is shorthand for the ratio of $f(x)$ and the sum of $f'''(\\epsilon_1)+ f'''(\\epsilon_2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use shorthand here because because we are not going to approximate $f'''$ (we are already approximating $f'$), so there is no point in writing it out.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the shorthand "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle \\left(\\frac{f}{f'''}\\right)^{1/3}=x_c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the curvature scale, or characteristic scale of the function $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several algorithms for choosing an optimal scale.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The better the scale chosen, the more accurate the approximation is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good rule of thumb, which is computationally quick, is to just use the absolute value of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_c = \\mid{x}\\mid$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we would use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h = \\epsilon_f^{1/3} \\mid{x}\\mid$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if $x$ is 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is simple to handle, we just add $\\epsilon_f^{1/3}$ to $x_c = \\mid x \\mid$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h = \\epsilon_f^{1/3} ( \\mid{x}\\mid + \\epsilon_f^{1/3})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Press et al. also suggest performing a final sequence of assignment operations that ensures $x$ and $x+h$ differ by an exactly representable number.  You assign $x+h$ to a temporary variable $temp$.  Then $h$ is assigned the value of $temp-h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using some dummy values for $x$ and $h$, the code would look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=32\n",
    "h=.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = x + h\n",
    "h = temp - x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Variance after Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply what we have learned to estimating $H(\\widehat{\\boldsymbol \\theta})^{-1}$ and $\\frac{\\delta}{\\delta \\widehat{\\boldsymbol{\\theta}}} g(\\widehat{\\boldsymbol{\\theta}})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pystan will provide an estimate of the constrained posterior mode $g(\\widehat{\\boldsymbol \\theta})$ by using the **optimizing()** function.  We'll refer to this value as **opt_parameters**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to get started on estimating $H(\\widehat{\\boldsymbol \\theta})^{-1}$ we have to call PyStan's **sampling()** function.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 1 iteration and specify **[opt_parameters]** in the **init** option and use \"Fixed_param\" in the **algorithm** option.  Let's call the object that **sampling()** returns **samp**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will allow us to compute the probability and gradient for the unconstrained posterior mode $\\widehat{\\boldsymbol \\theta}$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the unconstrained posterior mode, $\\widehat{\\boldsymbol \\theta}$ by using the **unconstrain_pars()** function with argument **opt_parameters** on **samp**.  We refer to this value as **unconstrained_pars**.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **grad_log_prob()** function can be applied to **samp** with argument **unconstrained_pars** to get the score function for the unconstrained parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **constrain_pars()** function can be used to compute the constrained parameters for any given input unconstrained parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loop over the unconstrained parameters $\\theta_1,\\ldots,\\theta_K$.  \n",
    "\n",
    "For iteration $i$, we calculate the bandwidth $h_i$ for parameter $\\theta_i$, as discussed in the previous section with $x=\\theta_i$.  We also create two copies of the **unconstrained_pars** vector, **parm_plus** and **parm_minus**.\n",
    "\n",
    "**parm_plus** is updated by adding $h_i$ to $\\theta_i$ and **parm_minus** is updated by subtracting $h_i$ from $\\theta_i$.\n",
    "Then we use the **grad_log_prob()** function to compute the score at **parm_plus** and the score at **parm_minus**.  \n",
    "These are stored in **grad_plus** and **grad_minus**.  Column $i$ of the Hessian is computed as (**grad_plus**-**grad_minus**)/2$h_i$.  \n",
    "\n",
    "We use the **constrain_pars** function to compute the constrained parameters at **parm_plus** and the constrained parameters at **par_minus**.  These are stored as **cons_plus** and **cons_minus**.  Column $i$ of $\\frac{\\delta}{\\delta \\widehat{\\boldsymbol{\\theta}}} g(\\widehat{\\boldsymbol{\\theta}})$ is computed as (**cons_plus**-**cons_minus**)/2$h_i$.  \n",
    "\n",
    "After looping over the parameters, we compute the matrix product $\\left\\{\\frac{\\delta}{\\delta \\widehat{\\boldsymbol{\\theta}}} g(\\widehat{\\boldsymbol{\\theta}})\\right\\}^T \\left\\{-H(\\widehat{\\boldsymbol{\\theta}})^{-1}\\right\\} \\left\\{\\frac{\\delta}{\\delta \\widehat{\\boldsymbol{\\theta}}} g(\\widehat{\\boldsymbol{\\theta}})\\right\\}$, our estimate of the covariance of $\\boldsymbol{\\theta_{c}}= g(\\boldsymbol \\theta)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a Python function that will get this done.  This functionality may be broken into several functions later, and customization in the optimization will be provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystan\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_map(stan_model,stan_data):\n",
    "    sm = stan_model\n",
    "    optimizing_fit = sm.optimizing(stan_data,algorithm=\"Newton\",init='0')\n",
    "    opt_parameters = optimizing_fit\n",
    "    parkeys = list(opt_parameters.keys())\n",
    "    test_grad_fit = sm.sampling(stan_data,              \\\n",
    "        iter=1,chains=1,warmup=0,init=[opt_parameters],check_hmc_diagnostics=False, \\\n",
    "                                algorithm=\"Fixed_param\")\n",
    "    unconstrained_pars = test_grad_fit.unconstrain_pars(opt_parameters)\n",
    "    nparm = unconstrained_pars.shape[0]\n",
    "    # note that constrained and unconstrained might have different dimensions.\n",
    "    nparmc = len(parkeys)\n",
    "    pars = unconstrained_pars\n",
    "    \n",
    "\n",
    "    Hessian = 0*np.ones((nparm,nparm))\n",
    "    Delta = 0*np.ones((nparmc,nparm))\n",
    "    epsdouble = np.finfo(float).eps\n",
    "    epsdouble = epsdouble**(1/3)\n",
    "    for i in np.arange(0,nparm):\n",
    "        scaleparm = abs(pars[i])\n",
    "        scale = epsdouble*(scaleparm+epsdouble)\n",
    "        scaleparmstable = scaleparm+scale\n",
    "        scale = scaleparmstable-scaleparm\n",
    "        parmplus = pars.copy()\n",
    "        parmminus = pars.copy()\n",
    "        parmplus[i]=parmplus[i]+scale\n",
    "        parmminus[i]=parmminus[i]-scale\n",
    "        gradplus = test_grad_fit.grad_log_prob(parmplus)\n",
    "        gradminus = test_grad_fit.grad_log_prob(parmminus)\n",
    "        Hessian[:,i] = (gradplus-gradminus)/(2*scale)\n",
    "        consplus = test_grad_fit.constrain_pars(parmplus)\n",
    "        consminus = test_grad_fit.constrain_pars(parmminus)\n",
    "        Delta[:,i] = (consplus[0:nparmc]-consminus[0:nparmc])/(2*scale)\n",
    "\n",
    "    iHessian = -np.linalg.inv(Hessian)\n",
    "    Cov = iHessian\n",
    "    Cov = np.matmul(np.matmul(Delta,Cov),Delta)\n",
    "    return {'mode':opt_parameters,'Cov':Cov,\"stan_model\":sm}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can provide users standard deviations based on this estimated covariance.  This will provide measures of variability for the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Confidence Intervals after Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the posterior mode, variance, and normal approximation to the posterior.  It is simple to create confidence (credible) intervals for the parameters.\n",
    "\n",
    "Let's talk a little bit about what these intervals are.\n",
    "\n",
    "For the parameter $\\gamma$ we want a $(1-\\alpha)%$ interval $(u,l)$ (defined on the observed data generated by a realization of $\\gamma$) to be defined such that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle \\mbox{Pr}(\\gamma \\in (u,l)) = 1-\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequentist confidence interval does not meet this criteria.  $\\gamma$ is just one fixed value, so it is either in the interval, or it isn't!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A credible interval (Bayesian confidence interval) can meet this criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we are able to use the normal approximation for $\\gamma \\vert \\bf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle \\gamma\\vert{\\bf{x}} \\approx_D N(\\hat\\gamma,\\hat\\sigma_\\gamma^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle \\begin{align}\n",
    "1-\\alpha &= \\mbox{Pr}(l  \\leq \\gamma \\leq u \\vert {\\bf x}) \\\\\n",
    "&= \\mbox{Pr}(l - \\hat{\\gamma} \\leq \\gamma - \\hat{\\gamma} \\leq u - \\hat{\\gamma} \\vert {\\bf x}) \\\\\n",
    "&= \\mbox{Pr}\\left(\\frac{l - \\hat{\\gamma}}{\\hat\\sigma_\\gamma} \\leq \\frac{\\gamma - \\hat{\\gamma}}{\\hat\\sigma_\\gamma} \\leq \\frac{u - \\hat{\\gamma}}{\\hat\\sigma_\\gamma} \\vert {\\bf x}\\right) \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, $\\displaystyle \\frac{\\gamma - \\hat{\\gamma}}{\\hat\\sigma_\\gamma^2}$  is $N(0,1)$, standard normal.  So we can use the standard normal quantiles in solving for $l$ and $u$.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The upper $\\alpha/ 2$ quantile of the standard normal distribution, $z_{\\alpha/ 2}$ satisfies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle \\mbox{Pr}(Z \\geq z_{\\alpha/ 2}) = \\alpha / 2$ \n",
    "\n",
    "for standard normal $Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noting that the standard normal is symmetric, if we can find $l$ and $u$ to satisfy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle \\frac{l - \\hat{\\gamma}}{\\hat\\sigma_\\gamma} = - z_{\\alpha/ 2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle \\frac{u - \\hat{\\gamma}}{\\hat\\sigma_\\gamma} = z_{\\alpha/ 2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we have a valid Bayesian confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple calculation shows that the solutions are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle l = - z_{\\alpha/ 2}\\hat\\sigma_\\gamma + \\hat{\\gamma}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle u = z_{\\alpha/ 2}\\hat\\sigma_\\gamma + \\hat{\\gamma}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $z_{\\alpha/ 2}$ quantile can be easily generated using **scipy.stats**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also adjust the intervals for inference on many parameters by using Bonferroni correction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know how to estimate the posterior variance after computing the posterior mode.  We also know how confidence intervals are made based on this posterior variance, mode, and the normal approximation to the posterior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform a simulation to corroborate our results and show how they can be used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see that we can properly estimate the posterior mode and variance, and use the normal approximation for the posterior distribution to generate confidence intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystan\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as spstats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our Stan specification for the model.  This a linear regression with a $\\chi^2$ prior for the intercept $\\alpha$ and a normal prior for the slope $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_a0d1a455dfb70045e204be5bae7f04b9 NOW.\n"
     ]
    }
   ],
   "source": [
    "model = \"\"\"\n",
    "data {\n",
    "    int<lower=0> N;\n",
    "    vector[N] x;\n",
    "    vector[N] y;\n",
    "}\n",
    "\n",
    "parameters {\n",
    "    real<lower=0> alpha;\n",
    "    real beta;\n",
    "}\n",
    "\n",
    "model {\n",
    "    alpha ~ chi_square(4);\n",
    "    beta ~ normal(1,1);\n",
    "    y ~ normal(alpha + beta * x,1);\n",
    "}\n",
    "\"\"\"\n",
    "stan_model = pystan.StanModel(model_code=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a constrained parameter and different types of prior distributions here.  This should provide robust corroboration that our methods work.  Note again that the priors are proper.  This is a requirement for our asymptotic approximations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a simulation function that we can call to draw from the model.  It takes as argument the sample size **n** and a random state **rs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(n,rs):\n",
    "     \n",
    "    # draw parameters from priors\n",
    "    beta = rs.normal(1.0,1)\n",
    "    alpha = rs.chisquare(4)\n",
    "    # draw observed data based on drawn parameters\n",
    "    x = rs.normal(size=n)\n",
    "    y = alpha + beta * x\n",
    "    y = rs.normal(y,1)\n",
    "    data = {'N': n, 'x': x, 'y': y}\n",
    "    return (data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we obtain a simulated draw from the model, we want to check the distribution of the posterior.  \n",
    "\n",
    "If we perform multiple draws from the original model and evaluate them marginally over the draws, we **are** evaluating the posterior.  \n",
    "\n",
    "As we demonstrated in previous notebooks, this can help us determine misspecification of the posterior. \n",
    "\n",
    "It will not let us directly evalute the posterior distribution though, because we are averaging over multiple draws of the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for each draw from the original model, we will use MCMC to perform posterior draws, which will then be used to evaluate\n",
    "the posterior distribution.\n",
    "\n",
    "The draws provide a posterior sample of $\\alpha$ and $\\beta$. \n",
    "\n",
    "As mentioned in previous notebooks, the posterior sample may need to be thinned to reduce autocorrelation and obtain an independent sample.\n",
    "\n",
    "Once thinning is completed, we create another parameter, $s_{\\alpha\\beta} = \\alpha + \\beta$.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each posterior sample, we record the mean and standard deviation of $\\alpha$, $\\beta$, and $s_{\\alpha\\beta}$ as $\\mu_\\alpha$, $\\mu_\\beta$, and $\\mu_{s_{\\alpha\\beta}}$ and $\\sigma_\\alpha$, $\\sigma_\\beta$, and $\\sigma_{s_{\\alpha\\beta}}$.\n",
    "\n",
    "The standard deviation of $s_{\\alpha\\beta}$, $\\sigma_{s_{\\alpha\\beta}}$ is the square-root of  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mbox{Var}(\\alpha+\\beta) = \\mbox{Var}(\\alpha) + \\mbox{Var}(\\beta) + 2 \\mbox{Cov}(\\alpha,\\beta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So evaluating the standard deviation of $s_{\\alpha\\beta}$ will help us evaluate the covariance of $\\alpha$ and $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also record our estimates of the posterior mode and variance that we derived using numerical optimization and the approximation methods discussed above.  These are $\\hat\\mu_\\alpha$, $\\hat\\mu_\\beta$, and $\\hat\\mu_{s_{\\alpha\\beta}}$ and $\\hat\\sigma_\\alpha$, $\\hat\\sigma_\\beta$, and $\\hat\\sigma_{s_{\\alpha\\beta}}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the mean and mode of the posterior distribution should be equal if it is normal.  \n",
    "\n",
    "So it is reasonable to use $\\mu$ to denote both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our estimates are accurate, and the normal approximation holds, then we should have the following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle \\frac{\\alpha-\\hat\\mu_\\alpha}{\\hat\\sigma_\\alpha}\\sim N(0,1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle \\frac{\\beta-\\hat\\mu_\\beta}{\\hat\\sigma_\\beta}\\sim N(0,1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle \\frac{s_{\\alpha\\beta}-\\hat\\mu_{s_{\\alpha\\beta}}}{\\hat\\sigma_{s_{\\alpha\\beta}}}\\sim N(0,1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check this assumption using an Anderson-Darling test. We store a record of whether the test rejects normality at the .05 significance level.  These rejection indicators are $r_\\alpha$, $r_\\beta$, and $r_{s_{\\alpha\\beta}}$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also generate 95% confidence intervals for each parameter.  We will check the coverage of the intervals by recording the fraction of the thinned MCMC samples that are cotained within.  This will also check the normal approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the methods discussed in the last section, the interval endpoints are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle l_\\alpha = - z_{.025}\\hat\\sigma_\\alpha + \\hat\\mu_{\\alpha}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle u_\\alpha = z_{.025}\\hat\\sigma_\\alpha + \\hat\\mu_{\\alpha}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle l_\\beta = - z_{.025}\\hat\\sigma_\\beta + \\hat\\mu_{\\beta}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle u_\\beta = z_{.025}\\hat\\sigma_\\beta + \\hat\\mu_{\\beta}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle l_{s_{\\alpha\\beta}} = - z_{.025}\\hat\\sigma_{s_{\\alpha\\beta}} + \\hat\\mu_{{s_{\\alpha\\beta}}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle u_{s_{\\alpha\\beta}} = z_{.025}\\hat\\sigma_{s_{\\alpha\\beta}} + \\hat\\mu_{{s_{\\alpha\\beta}}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function performs the posterior sampling and returns the results.  **L** denotes the number of posterior samples to draw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_results(mode,Cov,stan_model,stan_data,L,rs):\n",
    "    samptest_grad_fit = stan_model.sampling(stan_data,              \\\n",
    "        iter=3000,chains=1,warmup=1000,init=[mode],seed=rs,check_hmc_diagnostics=False)        \n",
    "    summary = pd.DataFrame(samptest_grad_fit.summary()['summary'], \\\n",
    "                           columns=samptest_grad_fit.summary()['summary_colnames'])\n",
    "    n_eff = np.min(summary['n_eff'])\n",
    "    N = samptest_grad_fit.sim[\"iter\"]\n",
    "    N_skip = np.array([np.ceil(N / n_eff)]).astype(int)[0]\n",
    "    N_needed = N_skip * L + samptest_grad_fit.sim[\"warmup\"]\n",
    "    if N_needed > N:\n",
    "            samptest_grad_fit = stan_model.sampling(stan_data,              \\\n",
    "                iter=N_needed,chains=1,warmup=1000,init=[mode],seed=rs,check_hmc_diagnostics=False)        \n",
    "\n",
    "    df = samptest_grad_fit.extract()\n",
    "    sampalpha  = np.array(df['alpha'][::(N_skip)])\n",
    "    sampalpha = sampalpha[0:L]\n",
    "    sampbeta  = np.array(df['beta'][::(N_skip)])\n",
    "    sampbeta = sampbeta[0:L]\n",
    "    sampsum = sampalpha + sampbeta\n",
    "    meanalpha = np.mean(sampalpha)\n",
    "    meanbeta = np.mean(sampbeta)\n",
    "    meansum = np.mean(sampsum)\n",
    "    sealpha = np.std(sampalpha)\n",
    "    sebeta = np.std(sampbeta)\n",
    "    sesum = np.std(sampsum)\n",
    "    adalpha = spstats.anderson((sampalpha-mode['alpha'])/np.sqrt(Cov[0,0]),'norm')\n",
    "    adalpha = adalpha[1][2] < adalpha[0]\n",
    "    adbeta = spstats.anderson((sampbeta-mode['beta'])/np.sqrt(Cov[1,1]),'norm')\n",
    "    adbeta = adbeta[1][2] < adbeta[0]\n",
    "    \n",
    "    sesumhat  = np.sqrt(np.matmul(np.ones((1,2)),np.matmul(Cov,np.ones((2,1)))))\n",
    "    sumest = (sampsum-  (mode['alpha'] + mode['beta']))/sesumhat\n",
    "    sumest = sumest.flatten()\n",
    "    adsum = spstats.anderson(sumest,'norm')\n",
    "    adsum = adsum[1][2] < adsum[0]\n",
    "    \n",
    "    mean = np.array([meanalpha,meanbeta,meansum])\n",
    "    se = np.array([sealpha,sebeta,sesum])\n",
    "    ad = np.array([adalpha,adbeta,adsum])\n",
    "    \n",
    "    z025 = spstats.norm.ppf(1-.025)\n",
    "    \n",
    "    lalpha = -z025*np.sqrt(Cov[0,0]) + mode['alpha']\n",
    "    ualpha = z025*np.sqrt(Cov[0,0]) + mode['alpha']\n",
    "    lbeta = -z025*np.sqrt(Cov[1,1]) + mode['beta']\n",
    "    ubeta = z025*np.sqrt(Cov[1,1]) + mode['beta']\n",
    "    lsum = -z025*sesumhat + mode['alpha'] + mode['beta']\n",
    "    usum = z025*sesumhat + mode['alpha'] + mode['beta']\n",
    "    propalpha = np.mean(np.less_equal(sampalpha,ualpha)*np.greater_equal(sampalpha,lalpha))\n",
    "    propbeta = np.mean(np.less_equal(sampbeta,ubeta)*np.greater_equal(sampbeta,lbeta))\n",
    "    propsum = np.mean(np.less_equal(sampsum,usum)*np.greater_equal(sampsum,lsum))\n",
    "    prop = np.array([propalpha,propbeta,propsum])         \n",
    "    return {'mu':mean, 'sigma': se, 'reject': ad, 'prop': prop}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform the simulation.  We use an observed data sample size of **n=600** based on **draws=1000** prior draws.  For each observed data sample we will use **L=500** posterior draws to evaluate the confidence intervals and posterior normal approximation.  We store the results in the **results** matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 600\n",
    "draws = 1000\n",
    "L=500\n",
    "rs = np.random.RandomState(218409)\n",
    "results = 0*np.ones((draws,18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(0, draws):\n",
    "    print(i)\n",
    "    data = draw(n,rs)\n",
    "    est_fit = estimate_map(stan_model,data)\n",
    "    post = posterior_results(est_fit['mode'],est_fit['Cov'],est_fit['stan_model'],data,L,rs)\n",
    "    results[i,0]= est_fit['mode']['alpha']\n",
    "    results[i,1] = post['mu'][0]\n",
    "    results[i,2] = np.sqrt(est_fit['Cov'][0,0])\n",
    "    results[i,3] = post['sigma'][0]\n",
    "\n",
    "    results[i,4]= est_fit['mode']['beta']\n",
    "    results[i,5] = post['mu'][1]\n",
    "    results[i,6] = np.sqrt(est_fit['Cov'][1,1])\n",
    "    results[i,7] = post['sigma'][1]\n",
    "    \n",
    "    results[i,8]= est_fit['mode']['alpha'] + est_fit['mode']['beta']\n",
    "    results[i,9] = post['mu'][2]\n",
    "    results[i,10] = np.sqrt(np.matmul(np.ones((1,2)),np.matmul(est_fit['Cov'],np.ones((2,1)))))\n",
    "    results[i,11] = post['sigma'][2]\n",
    "    \n",
    "    results[i,12] = post['reject'][0]\n",
    "    results[i,13] = post['reject'][1]\n",
    "    results[i,14] = post['reject'][2]\n",
    "    results[i,15] = post['prop'][0]\n",
    "    results[i,16] = post['prop'][1]\n",
    "    results[i,17] = post['prop'][2]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we summarize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>SD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alpha Mode</th>\n",
       "      <td>4.170080</td>\n",
       "      <td>3.073904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alpha Mean Posterior</th>\n",
       "      <td>4.170141</td>\n",
       "      <td>3.073850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SE Alpha Estimate</th>\n",
       "      <td>0.040828</td>\n",
       "      <td>0.000207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alpha SD Posterior</th>\n",
       "      <td>0.040723</td>\n",
       "      <td>0.001481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beta Mode</th>\n",
       "      <td>0.996798</td>\n",
       "      <td>1.009497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beta Mean Posterior</th>\n",
       "      <td>0.996799</td>\n",
       "      <td>1.009507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SE Beta Estimate</th>\n",
       "      <td>0.040869</td>\n",
       "      <td>0.001188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beta SD Posterior</th>\n",
       "      <td>0.040815</td>\n",
       "      <td>0.001952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sum Mode</th>\n",
       "      <td>5.166878</td>\n",
       "      <td>3.236201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sum Mean Posterior</th>\n",
       "      <td>5.166940</td>\n",
       "      <td>3.236160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SE Sum Estimate</th>\n",
       "      <td>0.057663</td>\n",
       "      <td>0.001425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sum SD Posterior</th>\n",
       "      <td>0.057553</td>\n",
       "      <td>0.002457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alpha AD Reject Rate</th>\n",
       "      <td>0.058000</td>\n",
       "      <td>0.233743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beta AD Reject Rate</th>\n",
       "      <td>0.051000</td>\n",
       "      <td>0.219998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sum AD Reject Rate</th>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.222027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alpha CI Proportion</th>\n",
       "      <td>0.950476</td>\n",
       "      <td>0.010601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beta CI Proportion</th>\n",
       "      <td>0.949680</td>\n",
       "      <td>0.010935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sum CI Proportion</th>\n",
       "      <td>0.950114</td>\n",
       "      <td>0.010711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Mean        SD\n",
       "Alpha Mode            4.170080  3.073904\n",
       "Alpha Mean Posterior  4.170141  3.073850\n",
       "SE Alpha Estimate     0.040828  0.000207\n",
       "Alpha SD Posterior    0.040723  0.001481\n",
       "Beta Mode             0.996798  1.009497\n",
       "Beta Mean Posterior   0.996799  1.009507\n",
       "SE Beta Estimate      0.040869  0.001188\n",
       "Beta SD Posterior     0.040815  0.001952\n",
       "Sum Mode              5.166878  3.236201\n",
       "Sum Mean Posterior    5.166940  3.236160\n",
       "SE Sum Estimate       0.057663  0.001425\n",
       "Sum SD Posterior      0.057553  0.002457\n",
       "Alpha AD Reject Rate  0.058000  0.233743\n",
       "Beta AD Reject Rate   0.051000  0.219998\n",
       "Sum AD Reject Rate    0.052000  0.222027\n",
       "Alpha CI Proportion   0.950476  0.010601\n",
       "Beta CI Proportion    0.949680  0.010935\n",
       "Sum CI Proportion     0.950114  0.010711"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ResultSummary = pd.DataFrame((np.concatenate(( \\\n",
    "                                np.mean(results,axis=0)[:,np.newaxis], \\\n",
    "                                np.std(results,axis=0)[:,np.newaxis]), \\\n",
    "                                             axis=1)),index=[          \\\n",
    "    \"Alpha Mode\",\"Alpha Mean Posterior\", \"SE Alpha Estimate\",\"Alpha SD Posterior\", \\\n",
    "    \"Beta Mode\",\"Beta Mean Posterior\", \"SE Beta Estimate\",\"Beta SD Posterior\", \\\n",
    "    \"Sum Mode\",\"Sum Mean Posterior\", \"SE Sum Estimate\",\"Sum SD Posterior\", \\\n",
    "     \"Alpha AD Reject Rate\", \"Beta AD Reject Rate\",\"Sum AD Reject Rate\", \\\n",
    "     \"Alpha CI Proportion\", \"Beta CI Proportion\", \"Sum CI Proportion\"], \\\n",
    "                                             columns=['Mean',\"SD\"])\n",
    "ResultSummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modes are close the posterior means on average.  The same is true for the posterior standard deviation and estimated standard error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rejection rates are close to .05, and the proportions of the samples in the 95% confidence intervals are close to .95  We can perform a hypothesis test of whether the rejection rate is .05 by checking whether .05 is in the confidence interval for the proportion.  We can test whether the proportions are .95 in an analogous manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lower</th>\n",
       "      <th>Upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alpha AD Reject Rate</th>\n",
       "      <td>0.044333</td>\n",
       "      <td>0.074336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beta AD Reject Rate</th>\n",
       "      <td>0.038205</td>\n",
       "      <td>0.066513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sum AD Reject Rate</th>\n",
       "      <td>0.039077</td>\n",
       "      <td>0.067635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alpha CI Proportion</th>\n",
       "      <td>0.935145</td>\n",
       "      <td>0.963078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beta CI Proportion</th>\n",
       "      <td>0.934250</td>\n",
       "      <td>0.962386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sum CI Proportion</th>\n",
       "      <td>0.934738</td>\n",
       "      <td>0.962764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Lower     Upper\n",
       "Alpha AD Reject Rate  0.044333  0.074336\n",
       "Beta AD Reject Rate   0.038205  0.066513\n",
       "Sum AD Reject Rate    0.039077  0.067635\n",
       "Alpha CI Proportion   0.935145  0.963078\n",
       "Beta CI Proportion    0.934250  0.962386\n",
       "Sum CI Proportion     0.934738  0.962764"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.stats.proportion as smp\n",
    "\n",
    "lower, upper = smp.proportion_confint(np.sum(results[:,12:],axis=0), \\\n",
    "                                      draws, method='beta')\n",
    "CiResults = pd.DataFrame(np.concatenate((lower[:,np.newaxis], \\\n",
    "                                         upper[:,np.newaxis]),axis=1), \\\n",
    "                                         index=[\"Alpha AD Reject Rate\", \"Beta AD Reject Rate\",\"Sum AD Reject Rate\", \\\n",
    "     \"Alpha CI Proportion\", \"Beta CI Proportion\", \"Sum CI Proportion\"], \\\n",
    "                                         columns = ('Lower', 'Upper'))\n",
    "CiResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".05 is in all the rejection rate intervals.  .95 is in the all of the confidence interval proportion intervals.\n",
    "\n",
    "Our simulation was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
